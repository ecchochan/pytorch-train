{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the preprocessing script below\n",
    "\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "def convert(data, tokenizer, max_seq_length, prefix):\n",
    "    '''\n",
    "    Convert the data and encoding the batch, and then shuffle the data and finally return binary file format\n",
    "        \n",
    "    Args:\n",
    "        data (list of list): list of list of firstSentence, secondSentence and ground-truth label\n",
    "                            data = [\n",
    "                                    [\"First sentence\", \"Second Sentence\", 0],\n",
    "                                    ...\n",
    "                                ]\n",
    "        \n",
    "        tokenizer (object of class Cantokenizer): tokenizing cantonese characters\n",
    "        max_seq_length (int): maximum length of the sentence\n",
    "        prefix (string): labelling the data classes into training class or testing class\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "    '''\n",
    "    # Encoding the two sentences\n",
    "    print(\"samples of input\")\n",
    "    print(json.dumps(random.sample([(e[0], e[1]) for e in data], 10), indent=2))\n",
    "    print()\n",
    "    print(\"samples of encoded\")\n",
    "    encodeds = tokenizer.encode_batch([(e[0], e[1]) for e in data])\n",
    "    print(encodeds[:10])\n",
    "    print()\n",
    "    # Initializing empty lists for holding different data\n",
    "    data_original = []\n",
    "    data_attn_mask = []\n",
    "    data_labels = []\n",
    "    data_type_ids = []\n",
    "    \n",
    "    # For every encoded data (encodeds) and original data (data)\n",
    "    for e, t in zip(encodeds, data):\n",
    "        # Now e is the encoded values, t is the training/testing data set\n",
    "        # Check the ground-truth label and assign as label\n",
    "        label = t[2]\n",
    "        # If the length of encoded id is longer than the maximum sequence length, skip processing the data\n",
    "        if len(e.ids) > max_seq_length:\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        e.pad(max_seq_length)\n",
    "        # Append the data into list to store the values\n",
    "        data_original.append(e.ids)\n",
    "        data_attn_mask.append(e.attention_mask)\n",
    "        data_labels.append(label)\n",
    "        data_type_ids.append(e.type_ids)\n",
    "    \n",
    "    # Create a list of indices map the indices in the original data list\n",
    "    indices = list(range(len(data_original)))\n",
    "    \n",
    "    # Randomly shuffle the indices to prevent data aggregation leading to overfitting towards one class\n",
    "    random.shuffle(indices)\n",
    "    \n",
    "    # Override the original lists using the shuffled list \n",
    "    data_original = [data_original[i] for i in indices]\n",
    "    data_attn_mask = [data_attn_mask[i] for i in indices]\n",
    "    data_labels = [data_labels[i] for i in indices]\n",
    "    data_type_ids = [data_type_ids[i] for i in indices]\n",
    "    \n",
    "    # Cast the type of the data elements into specified type \n",
    "    ids = np.array(data_original).astype(np.int16)\n",
    "    attn = np.array(data_attn_mask).astype(np.int8)\n",
    "    labels = np.array(data_labels).astype(np.int8)\n",
    "    type_ids = np.array(data_type_ids).astype(np.int8)\n",
    "\n",
    "    print(f\"writing to {prefix}_xx\")\n",
    "    \n",
    "    # Writing the data into binary file format\n",
    "    with open(prefix+\"_ids\", 'wb') as f:\n",
    "        f.write(ids.tobytes())\n",
    "    with open(prefix+\"_mask\", 'wb') as f:\n",
    "        f.write(attn.tobytes())\n",
    "    with open(prefix+\"_type_ids\", 'wb') as f:\n",
    "        f.write(type_ids.tobytes())\n",
    "    with open(prefix+\"_label\", 'wb') as f:\n",
    "        f.write(labels.tobytes())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Load the json file of final training and testing datasets.\n",
    "'''\n",
    "import json\n",
    "# Load json file into python's object\n",
    "with open('final_train_set.json') as f:\n",
    "    final_train_set = json.load(f)\n",
    "with open('final_test_set.json') as f:\n",
    "    final_test_set = json.load(f)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Import the cantokenizer and convert the training and testing dataset into binary file format.\n",
    "'''\n",
    "\n",
    "import json\n",
    "\n",
    "from cantokenizer import CanTokenizer, NORM_OPTIONS\n",
    "\n",
    "\n",
    "tokenizer = CanTokenizer('cantokenizer-vocab.txt', \n",
    "                         add_special=True, \n",
    "                         add_special_cls='<s>', \n",
    "                         add_special_sep='</s>',\n",
    "                         norm_options = NORM_OPTIONS.ZH_NORM_MAPPING | \n",
    "                                        NORM_OPTIONS.SIMPL_TO_TRAD | \n",
    "                                        NORM_OPTIONS.SEPARATE_INTEGERS | \n",
    "                                        NORM_OPTIONS.SEPARATE_SYMBOLS\n",
    "                        )\n",
    "\n",
    "    \n",
    "    \n",
    "# Define maximum sequence length\n",
    "max_seq_length = 96\n",
    "\n",
    "# If there are both first sentence and second sentence (no missing value), thne we can convert the string into\n",
    "# binary files\n",
    "convert([e for e in final_train_set if e[0] and e[1]], tokenizer, 96, 'train')\n",
    "\n",
    "convert([e for e in final_test_set if e[0] and e[1]], tokenizer, 96, 'test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# We will train the model here -> https://colab.research.google.com/drive/13nW5HluBSDaQVlck5V4cbiuP9OzA7ubi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
